import streamlit as st
def display(tab):
   tab.header("Abstract")
   tab.write("In the field of polymer informatics, the availability of high-quality structured data for training machine learning models to predict polymer properties is a crucial problem.  Transferring knowledge from the data-rich molecule domain to polymers using deep neural networks is a promising avenue for overcoming this data scarcity.  In this study, we utilise different transfer learning techniques such as zero-shot learning, few-shot learning, fine-tuning, and frozen featurization to improve the prediction accuracy of the polymer band gap that determines the material's electrical conductivity and optical properties.")
   tab.write("We develop base machine learning models for molecules and polymers using three different fingerprinting techniques that numerically encode the polymers structure, namely Circular, PolyBERT, and MiniLM fingerprints. The Circular fingerprints consist of a vector made of manually selected features, the PolyBERT fingerprints are obtained using the transformer trained using polymer data where as the MiniLM fingerprints are obtained using sentence-transformers. The models for molecules serve as a starting point for training the transfer learning models, while the base models for polymers allow us to validate the performance improvements of our transfer learning approaches. We find that zero-shot and few-shot learning from molecules to polymers does not improve the prediction accuracy for the band gap, because the difference between molecules and polymers cannot be learned in zero or few epochs using the chosen ML architecture. On the contrary, the fine-tuning and frozen featurization transfer learning models show significant improvements in the prediction accuracy of the band gap. We conclude that retaining a few layers in frozen featurization or fine-tuning allows for learning the differences as well as similarities between the molecule and polymer domains.")
   tab.header("Transfer Learning")
   tab.write("Human beings have the tendency of transferring knowledge learned from one task to another task. For example, if we know how to ride a bicycle, we transfer the same skills to learning to ride a motorbike. In general usually we do not learn new tasks from scratch. We leverage the knowledge from our past learning to learn new tasks. It allows us to reuse our knowledge. This makes learning the new task easier and reduces the amount of time consumed to learn it. This concept can be extended to machine learning and deep learning. Most machine learning and deep learning algorithms are trained to solve specific tasks. Once the distribution of feature space changes, a new model needs to be trained regardless of the similarity between tasks. Transfer learning helps to bridge this gap by utilizing the knowledge learned from one task to solve another related task. The key motivation behind transfer learning is the shortage of labeled data. Training supervised deep learning networks require a vast amount of labeled data which can be difficult and time-consuming to obtain.")
   tab.write("Transfer learning techniques to develop and train a model are fundamentally similar to methods employed in traditional machine learning or deep learning. They are trained for specific tasks using specific datasets. No transferable knowledge is retained in traditional learning. However, transfer learning retains transferable knowledge such as features,weights, etc. from previously trained models. This knowledge is then used to train new models even if the new task does not have sufficient data. Knowledge learned from previously trained task acts as an additional input to train the new task.")

